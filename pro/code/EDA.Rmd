---
title: "EDA"
author: "Yunting Gu, Shuangyu Zhao, Wenting Liu"
date: "2023-03-16"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# library install
library(tidyverse)
library(sqldf)
library(ggplot2)
library(ggcorrplot)
```

Our project focus on two questions:
1. for the word with \r\, train the model to fine whether the speaker misses this pronounciation
2. according to the pronounciation, determine whether this words has \r\ or \t\

```{r}
Rclass <- readRDS("/Users/apple/Desktop/STT811_appl_stat_model/pro/data/RClassifierData_03July2019.Rds")
Tclass <- readRDS("/Users/apple/Desktop/STT811_appl_stat_model/pro/data/TClassifierData_14Nov2019.Rds")
```

see the size of 2 dataframe
```{r}
dim(Rclass)
```
```{r}
dim(Tclass)
```

1. EDA for question 1
```{r}
dim(Rclass)
```

```{r}
sqldf("SELECT DISTINCT Rpresent
       FROM Rclass")
```
```{r}
sqldf("SELECT COUNT(*)
      FROM Rclass
      WHERE Rpresent = 'Absent' ")
```
see the number of data have been coded 'present'(means pronoucing the \r\ ) and 'absent' (means missing the \r\ )
```{r}
sqldf("SELECT COUNT(*)
      FROM Rclass
      WHERE Rpresent = 'Present' ")
```
```{r}
sqldf("SELECT COUNT(*)
      FROM Rclass
      WHERE Rpresent = 'Absent' ")
```
```{r}
rdf_pre_ab <- sqldf("SELECT *
                    FROM Rclass
                    WHERE Rpresent = 'Absent' OR Rpresent = 'Present'")
```

```{r}
barplot(table(rdf_pre_ab$Rpresent)) 
```
we can see that the data are unbalanced.


extract the columns I think useful
```{r}
rdf_useful <- rdf_pre_ab[34:217]
rdf_useful$Rpresent <- rdf_pre_ab$Rpresent
rdf_useful <- na.omit(rdf_useful)
dim(rdf_useful)
```

encode absent - 0; present - 1
```{r}
rdf_useful$Rpresent_encode <- ifelse(rdf_useful$Rpresent == "Present", 1, 0)
```

standardize the data
```{r}
# Standardize the data
standardized_rdata <- scale(rdf_useful[, c(1:184)], center = TRUE, scale = TRUE)
rdf_useful[, c(1:184)] <- standardized_rdata
```

```{r}
write.csv(rdf_useful, "detectingr.csv")
```



the correlation between elements
```{r}
cor_rdf_usefurl <- round(cor(rdf_useful[, c(1:184, 186)]), 2)
write_csv(as.data.frame(cor_rdf_usefurl), file = "rdf_correlation.csv")
```

print all the variables name, whose correlations are higher than 0.9
```{r}
high_corr <- data.frame(var1 = character() , var2 = character(), cor = numeric())
z = 1
for (i in 1:185) {
  for (j in 1:i) {
    if (abs(cor_rdf_usefurl[i, j]) > 0.9 && i != j) {
      high_corr[z, 1] <- colnames(cor_rdf_usefurl)[j]
      high_corr[z, 2] <- colnames(cor_rdf_usefurl)[i]
      high_corr[z, 3] <- cor_rdf_usefurl[i, j]
      z = z +1
    }
  }
}
high_corr
```

For the variable listed in this dataframe, we are gonna choose var1 and drop var2, when we feed data into classifer.

for the correlation between variables and target
```{r}

barplot(abs(cor_rdf_usefurl[, 185]), main = "Correlation with target variable", ylab = "Absolute correlation")
abline(h = 0.5, col = "red")

```

we can see the correlation is not really high.

for the variable having highest correlation with target, we will observe the distribution of it.
```{r}
cor_rdf_usefurl_df <- as.data.frame(cor_rdf_usefurl)
head(cor_rdf_usefurl_df)
```
```{r}
cor_rdf_usefurl_df$row <- colnames(cor_rdf_usefurl_df)
df <- sqldf("SELECT row, Rpresent_encode
             FROM cor_rdf_usefurl_df
             ORDER BY Rpresent_encode DESC
             LIMIT 4")
df
```

```{r}
ggplot(data = rdf_useful, aes(x = intens_F3min)) + geom_histogram() + facet_grid(.~Rpresent)
```


2. EDA for question 2

```{r}
Rclass_token <- Rclass
Rclass_token$token <- 'r'
```

set column names to the lower case
```{r}
colnames(Rclass_token) <- tolower(colnames(Rclass_token))
colnames(Tclass) <- tolower(colnames(Tclass))

```


list the all same column in two files
```{r}
same <- Reduce(intersect, list(colnames(Rclass_token), colnames(Tclass)))
same
```

merge these two dataframe with same colums
```{r}
merge_df <- rbind(Rclass_token[, same], Tclass[, same])
merge_df <- na.omit(merge_df)
```
```{r}
barplot(table(merge_df$token))
```

the label are unbalanced
```{r}
head(merge_df)
```

```{r}
num_token <- select_if(merge_df, is.numeric)
num_token$token <- merge_df$token
head(num_token)
```

scale the numeric variable
```{r}
# Standardize the data
standardized_mergedata <- scale(num_token[, c(1:49)], center = TRUE, scale = TRUE)
num_token[, c(1:49)] <- standardized_mergedata
```

```{r}
write.csv()
```



encode the token. r-1, t-0
```{r}
num_token$token_encoded <- ifelse(num_token$token == 'r', 1, 0)
merge_cor <- cor(num_token[, c(1:49, 51)])
```

```{r}
write.csv(num_token, "distinguishr_t.csv")
```
correlation map between variables(use python to draw it)
```{r}
write_csv(as.data.frame(num_token[, c(1:49, 51)]), file = "merge_df.csv")
```

![](merge_Df_correlation2.png)
print all the variables name, whose correlations are higher than 0.9
```{r}
high_corr2 <- data.frame(var1 = character() , var2 = character(), cor = numeric())
z = 1
for (i in 1:50) {
  for (j in 1:i) {
    if (abs(merge_cor[i, j]) > 0.9 && i != j) {
      high_corr2[z, 1] <- colnames(merge_cor)[j]
      high_corr2[z, 2] <- colnames(merge_cor)[i]
      high_corr2[z, 3] <- merge_cor[i, j]
      z = z +1
    }
  }
}
high_corr2
```

for the correlation between variables and target
```{r}

barplot(abs(merge_cor[, 50]), main = "Correlation with target variable", ylab = "Absolute correlation")
abline(h = 0.5, col = "red")

```
```{r}
merge_cor_df <- as.data.frame(merge_cor)
head(merge_cor_df)
```
```{r}
merge_cor_df$row <- colnames(merge_cor_df)
sqldf("SELECT row, token_encoded
      FROM merge_cor_df
      ORDER BY token_encoded DESC
      LIMIT 4")
```

the distribution of variables with relatively high correlation with token
```{r}
ggplot(data = num_token, aes(x = tokennum)) + geom_histogram() + facet_grid(.~token)
```
```{r}
ggplot(data = num_token, aes(x = celexfreq)) + geom_histogram() + facet_grid(.~token)
```
